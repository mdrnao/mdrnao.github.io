{
  "hash": "24a2f777ea23ae01113288741e01924a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"PCA\"\ndescription: \"Reviewing the basics of Principle Component Analysis\"\nauthor: \n  - name: Holly Hall\n    url: https://mdrnao.github.io\ndate: 08-06-2024\ncategories: [ basics, PCA]\nbibliography: references.bib\nimage: image.png\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n## Purpose\n\nAs data gets complex with many samples/measurements, we end up with many dimensions to consider for a given dataset.\n\nPCA is a linear dimensionality reduction tool, utilising Singular Value Decomposition, to project many dimensions into a lower dimension space and therefore capture the most important information from it. PCA finds the best fitted line by maximising the sum of the squared distances between the projected points to the origin. It finds the best linear combination of variables to maximise distance of samples.\n\n## Practical example\n\nLet's use the popular Iris data set, with the goal of predicting iris species from sepal and petal information.\n\nHere, we have measurements of sepal length and width, as well as petal width and length, for three iris species.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  require(dplyr);require(ggplot2);require(cowplot)\n})\ndata(iris)\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\nWe can plot each variable against one another, to see how the relationship between the variables changes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(\n  \n  ggplot(iris, aes(Sepal.Length ,Sepal.Width )) +\n  geom_point() +\n  theme_classic() +\n  panel_border(color = \"black\") +\n  ggpubr::stat_cor() ,\n  ggplot(iris, aes(Petal.Length, Petal.Width )) +\n  geom_point() +\n  theme_classic() +\n  panel_border(color = \"black\")+\n  ggpubr::stat_cor() \n\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nHere we can see that Sepal length and width are not related, but petal length and width are. This gets tedious if we have many variables, which is where PCA comes in.\n\nA crucial first step is checking for missing values and normalisation of the data. The data should at least be centered, but can also be scaled. Scaling is more important if the measurements using different units/scales.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolSums(is.na(iris))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n           0            0            0            0            0 \n```\n\n\n:::\n\n```{.r .cell-code}\niris.list <- list(\n  raw = iris[,1:4],\n  scaled = scale(iris[,1:4]),\n  species = iris$Species\n  \n)\n```\n:::\n\n\n### Performing PCA \n\n\n::: {.cell}\n\n```{.r .cell-code}\npca = princomp(iris.list$scaled)\npca.ind = factoextra::get_pca_ind(pca)\npca.var = factoextra::get_pca_var(pca)\n\nsummary(pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          Comp.1    Comp.2     Comp.3      Comp.4\nStandard deviation     1.7026571 0.9528572 0.38180950 0.143445939\nProportion of Variance 0.7296245 0.2285076 0.03668922 0.005178709\nCumulative Proportion  0.7296245 0.9581321 0.99482129 1.000000000\n```\n\n\n:::\n:::\n\n\nFor components have been generated, which is equal to the number of variables of the data.\n\nIn PCA to split the covariance (or correlation) matrix into scale parts (eigenvalues) and direction (eigevectors). Eigenvectors with scale are loadings.\n\nEigenvector is just a coefficient of orthogonal *transformation* or projection, it is devoid of \"load\" within its value. \"Load\" is (information of the amount of) variance, magnitude. PCs are extracted to explain variance of the variables. Eigenvalues are the variances of (= explained by) PCs. When we multiply eigenvector by sq.root of the eivenvalue we \"load\" the bare coefficient by the amount of variance. By that virtue we make the coefficient to be the measure of *association*, co-variability.\n\nThe loadings can be considered the coefficients of the linear combination of the original variables in the dataset, from which the PCs are computed. Larger the magnitude of the values are, the more important it is to that component. Positive values indicate that it's presence is important, whereas negative values indicate its lack of presence is significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca$loadings\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLoadings:\n             Comp.1 Comp.2 Comp.3 Comp.4\nSepal.Length  0.521  0.377  0.720  0.261\nSepal.Width  -0.269  0.923 -0.244 -0.124\nPetal.Length  0.580        -0.142 -0.801\nPetal.Width   0.565        -0.634  0.524\n\n               Comp.1 Comp.2 Comp.3 Comp.4\nSS loadings      1.00   1.00   1.00   1.00\nProportion Var   0.25   0.25   0.25   0.25\nCumulative Var   0.25   0.50   0.75   1.00\n```\n\n\n:::\n:::\n\n\nIn this example, the first component has high positive values for everything apart from sepal width, which is relatively negative. The second component has high value for width. The first PC is the linear combination PC1 = 0.52\\*SepalLength â€“ 0.27\\*SepalWidth + 0.58\\*PetalLength + 0.56\\*PetalWidth. You can interpret this as a contrast between the SepalWidth variable and an equally weighted sum of the other variables.\n\n### Visualisation\n\n#### Scree plot\n\nWe can first look at a scree plot. This is used to visualise the importance of each PC and can be used to determine how many components (eigenvalues) you need for a reasonably accurate representation of the original data. This is important as the visualisation of 4+ variables is rather difficult!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute total variance\nvariance = pca$sdev^2 / sum(pca$sdev^2)\n\nrequire(ggplot2); require(cowplot)\ndata.frame(\n  PC = paste0(\"PC\", 1:4),\n  variance\n) %>%\n  mutate(cumSum = cumsum(variance)) %>%\n  ggplot(aes(PC, variance)) +\n  geom_hline(yintercept = 0.96, linetype = 2) +\n  geom_col() +\n  theme_classic() +\n  panel_border() +\n  geom_point(aes(y=cumSum),\n             size = 3, \n             shape = 15,\n             colour = \"deeppink\"\n             ) +\n  geom_line(aes(y=cumSum, group = 1),\n            colour = \"deeppink\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nEach bar is the amount of variance for each PC, and the points represent the cumulative sum of the variance. Here, we can see that the first two components capture 96% of the variance in the dataset together (dashed line).\n\n#### Profile plot\n\nThis gives the correlation of each of the original variables and their contribution to each dimension.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca.var$cor %>%\n  data.frame() %>%\n  tibble::rownames_to_column(\"Variable\") %>%\n  tidyr::pivot_longer(cols = 2:5) %>%\n  ggplot(aes(x=Variable, y=value, colour = name)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  theme_classic() +\n  panel_border(colour = \"black\") +\n  geom_point() +\n  geom_line(aes(group = name)) +\n  labs(y= \"Correlation\",\n       colour = \"PC\") +\n  scale_colour_manual(values = c(\"#D1495BFF\",\"#2E4057FF\",\"#66A182FF\",\"#F9771EFF\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nPC1 is highly correlated with petal length and width, as well as sepal length, but negatively correlated with sepal width. PC2 is highly correlated with sepal width, and to a lesser extent sepal length. The third and fourth have weak correlations.\n\nWe can also plot this as a pattern plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca.var$cor %>%\n  data.frame() %>%\n  tibble::rownames_to_column(\"Variable\") %>%\n  ggplot(aes(x=Dim.1, y=Dim.2)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2) +\n  theme_classic() +\n  panel_border(colour = \"black\") +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = Variable)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n#### Score plot\n\nThe score plot visualises the projection of the original data into the projected space, utilising the \"formula\" of each variable contribution to the PC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pca.ind$coord, aes(Dim.1, Dim.2, colour = iris.list$species)) +\n  geom_hline(yintercept = 0, linetype=2) +\n  geom_vline(xintercept = 0, linetype=2) +\n  geom_point() +\n  theme_classic() +\n  panel_border(colour = \"black\") +\n  labs(colour = \"Species\") +\n  scale_colour_manual(values = c(\"#2E4057FF\",\"#66A182FF\", \"#D1495BFF\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nWe can see that setosa species forms its own group to the left of PC1, with versicolor and virginica to the left. The loadings can also be plotted ontop of the scores plot, otherwise known as a biplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfactoextra::fviz_pca_biplot(pca)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n*More to come...*\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}