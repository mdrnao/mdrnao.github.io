{
  "hash": "6f04e8d16caec3ce7a4c98ab11d8117e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"UMAP\"\ndescription: \"Reviewing the basics of UMAP\"\nauthor: \n  - name: Holly Hall\n    url: https://mdrnao.github.io\ndate: 08-06-2024\ncategories: [ basics, UMAP]\nbibliography: references.bib\nimage: image.jpeg\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\nexecute: \n  cache: true\n---\n\n\n## Overview\n\nUMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that excels at reducing high-dimensional data into a low-dimensional representation. Unlike PCA (Principal Component Analysis), which is linear and only effective when the first few components capture a significant portion of the data's variation, UMAP can handle more complex, non-linear relationships within the data.\n\nUMAP operates by calculating similarity scores between pairs of data points in the original high-dimensional space. These similarity scores are determined based on the number of nearest neighbors each point has, capturing the local topology of the data. UMAP uses these scores to construct a fuzzy topological representation, which it then optimizes to preserve the structure of the data when it is projected into a lower-dimensional space.\n\nTo create the low-dimensional representation, UMAP initializes the process with a spectral embedding, a graph-based approximation, which serves as a starting point. It then iteratively adjusts the positions of the points, guided by low-dimensional similarity scores derived from a t-distribution curve, allowing UMAP to effectively maintain the local relationships of the data points.\n\nUMAP shares similarities with t-SNE (t-distributed Stochastic Neighbor Embedding), another popular dimensionality reduction technique. However, while t-SNE starts with a random initial representation and adjusts every point slightly during each iteration, UMAP's initial step is more structured, using a spectral embedding. UMAP also has the flexibility to move individual points or subsets of points with each iteration, making it particularly efficient for handling large datasets.\n\nOne of UMAP's strengths is its ability to balance local and global structure in the data. By adjusting the number of nearest neighbors (NN) used in the calculation, UMAP can either emphasize the local structure, revealing finer details within clusters with low NN, or capture broader trends across the dataset with higher NN, providing a more global perspective.\n\nThe proper description of how UMAP works can be found in the [documentation](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html), or a handy [YouTube](https://www.youtube.com/watch?v=nq6iPZVUxZU&t=1398s) video.\n\nA really good explanation of the cost functions and gradient descent differences between the various dimensional reduction tools can be found [here](https://jlmelville.github.io/smallvis/theory.html). And [this paper](https://www.jmlr.org/papers/volume22/20-1061/20-1061.pdf) gives an indepth comparison of the underlying maths, which I need to read properly.\n\n## Practical examples!\n\nDownload the popular mnist dataset, and make a basic umap plot with standard parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmnist <- snedata::download_mnist(\"https://ossci-datasets.s3.amazonaws.com/mnist/\")\n\nmnist <- list(\n  data = mnist[,1:784],\n  label = mnist$Label\n)\n\nsuppressMessages({library(rnndescent); require(uwot); require(ggplot2); require(cowplot); require(dplyr) })\n\nset.seed(42)\nmnist_umap2 <- umap2(mnist$data)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  mnist_umap2,\n  label = mnist$label\n) %>% \n  ggplot(aes(X1, X2, colour = label)) +\n  geom_point(size=0.5, alpha = 0.4) +\n  theme_classic()+\n  guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) +\n  panel_border(colour = \"black\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### number of NN\n\nSince UMAP uses the number of neighbours to a given data point to determine its position, the parameter n_neighbours is very important. I've not seen a principled way of determining the optimal number of neighbours yet, so I do it primarily through trial and error. Too few neighbours and you can lose the global structure, too many and you might lose some local structure.\n\nWe'll see how choosing 2, 5, 20, and 200 neighbours changes the UMAP representation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw_umap <- function(nn = 15) {\n  umap.res = umap2(mnist$data, n_neighbors =nn )\n  data.frame(\n    umap.res,\n    label = mnist$label\n  ) %>% \n    ggplot(aes(X1, X2, colour = label)) +\n    geom_point(size=0.5, alpha = 0.4) +\n    theme_classic()+\n    guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) +\n    panel_border(colour = \"black\") +\n    labs(subtitle = paste0(\"NN = \", i)) +\n    theme(axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          axis.text = element_blank())\n}\n\numap.nn <- list()\nfor(i in c(2,5,20,200)){\n  name = as.character(i)\n  umap.nn[[name]] <- draw_umap(i)\n  \n}\n\nplot_grid(plotlist = umap.nn)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWhat is clear is that only 2 neighbours have no clustering structure, and all the digits form one large cloud. 5 neighbours have more structure, but there seems to be more mixture of the clusters than 20 neighbours. At 200 it looks like we're starting to lose definition again.\n\n***TBC***\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}