{
  "hash": "c0707464d6439727da46f71fb3883138",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Random Forest Explainer\"\ndescription: \"Testing out randomForestExplainer package\"\nauthor: \n  - name: Holly Hall\n    url: https://mdrnao.github.io\ndate: 08-13-2024\ncategories: [machine learning, basics, explainer, TCGA, random forest]\nbibliography: references.bib\nimage: image.png\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n## Purpose\n\nPredictive models with complex structures can be effectively developed using random forest machine learning. A random forest is a type of predictive ML model that constructs a collection of decision trees, each generated from a bootstrapped sample of the data, to reach a consensus prediction. Random forests are versatile, applicable to both regression and classification tasks.\n\nHowever, they can sometimes operate as a \"black box,\" making it challenging to interpret their inner workings. To address this, it's valuable to examine the model to identify which variables most significantly contribute to prediction accuracy. Here, I'll be testing out the randomForestExplainer package.\n\n## Build a forest\n\nFirst we'll download some TCGA data, create our target predictor (1Y survival), and remove any genes which contribute nothing (0 counts all the way!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load in packages\nsuppressMessages({\n  require(recount3)\n  require(dplyr)\n  require(ggplot2)\n  require(cowplot)\n  require(fst)\n  require(randomForest)\n  require(randomForestExplainer)\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Get the project list from recount\nprojects_all <- rbind(\n  available_projects(organism = c(\"human\"))) %>%\n  filter(file_source == \"tcga\")\n\n  ## Get recount project information\n  proj_info <- projects_all %>%\n    filter(project == \"LAML\")\n  proj_info <- proj_info[1, ]\n\n  ## Download\n  rse <- create_rse(project_info = proj_info, type = \"gene\")\n\n  ## save information in list\n  raw_data <- list(\n    raw_counts = compute_read_counts(rse),\n    metadata = data.frame(colData(rse))\n  )\n  \n  raw_data$metadata_min <-\n    data.frame(\n      vital = raw_data$metadata$tcga.xml_vital_status,\n      days = raw_data$metadata$tcga.xml_days_to_death,\n      barcode = raw_data$metadata$tcga.tcga_barcode\n    ) %>%\n    mutate(vital_1y = ifelse(vital == \"Dead\" & \n                               days <= 365, \"Dead\", \"Alive\")) \n  \n  table(raw_data$metadata_min$vital_1y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAlive  Dead \n   80    35 \n```\n\n\n:::\n\n```{.r .cell-code}\n  colnames(raw_data$raw_counts) = raw_data$metadata$tcga.tcga_barcode\n  \n  remove = is.na(raw_data$metadata_min$vital_1y)\n  remove_counts = edgeR::filterByExpr(raw_data$raw_counts,\n                                      min.count = 500)\n  \n\n\nreduced_data = list(\n  raw_counts = raw_data$raw_counts[remove_counts,!remove],\n  metadata = raw_data$metadata_min[!remove, ]\n)\n\nlibrary('biomaRt')\nmart <- useDataset(\"hsapiens_gene_ensembl\", useMart(\"ensembl\"))\ngene_list <- getBM(filters= \"ensembl_gene_id\", \n                attributes= c(\"ensembl_gene_id\",\"hgnc_symbol\"),\n                values=limma::strsplit2(rownames(reduced_data$raw_counts), split = \"\\\\.\")[,1],\n                mart= mart)\n\ngene_list.o = data.frame(ensembl_gene_id = limma::strsplit2(rownames(reduced_data$raw_counts), split = \"\\\\.\")[,1] ) %>%\n  left_join(gene_list)\n\nrownames(reduced_data$raw_counts) = ifelse(gene_list.o$hgnc_symbol == \"\", \n                                           gene_list.o$ensembl_gene_id,\n                                           gene_list.o$hgnc_symbol)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf.data = data.frame(t(reduced_data$raw_counts))\nrf.data$vital = as.factor(reduced_data$metadata$vital_1y)\n\nset.seed(104)\n\nrf = randomForest(vital ~ ., \n                  data = rf.data,\n                  ntree = 1000,\n                  localImp = T)\n```\n:::\n\n\n### Error\n\nFirst, we'll look at the error rate from this model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf$err.rate %>%\n  data.frame() %>%\n  tibble::rowid_to_column() %>%\n  tidyr::pivot_longer(cols = 2:4) %>%\n  ggplot(aes(x=rowid, y=value, colour = name)) +\n  geom_line() +\n  theme_minimal() +\n  panel_border() +\n  labs(x=\"Trees\", \n       y=\"Error\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe OOB line is the out of bag error rate. Since a random forest is built using bootstrapped samples of the data, the OOB is the mean prediction error rate from each sample compared to the prediction without the sample present, therefore the error from unseen samples. the OBB error rate stabilises pretty quickly, so more trees will not improve the error rate.\n\nSimilarly, the error rates for both of the classes stabilise. However, the error for patients which are dead within the year is particularly high.\n\nLets have a look at the object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = vital ~ ., data = rf.data, ntree = 1000,      localImp = T) \n               Type of random forest: classification\n                     Number of trees: 1000\nNo. of variables tried at each split: 91\n\n        OOB estimate of  error rate: 32.17%\nConfusion matrix:\n      Alive Dead class.error\nAlive    73    7   0.0875000\nDead     30    5   0.8571429\n```\n\n\n:::\n:::\n\n\nHere, you can see that the OOB is approximately 30%, and the error rate for Dead patients is 0.8. This is a terrible model for prediction, as you can see from the predictions that the model typically just predicts a sample to be Alive.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(rf,\n           sort = T,\n           n.var = 10,\n           main = \"Top 10 - Variable Importance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Tree depth\n\nVariables which tend to sit closer to the root node tend to be more important for prediction accuracy. Therefore, assessing the minimal depth for each variable can be a useful exercise to examine a variables importance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmin_depth_frame <- min_depth_distribution(rf)\nplot_min_depth_distribution(min_depth_frame)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nTo look at the top three with the smallest depth:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(\n  data.frame(\n  gene = rf.data$STIM2,\n  class = rf.data$vital\n) %>%\n  ggplot(aes(x=class, y=gene))+\n  geom_boxplot() +\n  theme_minimal() +\n  panel_border() +\n  ggpubr::stat_compare_means() +\n  labs(subtitle = \"STIM2\"),\n\ndata.frame(\n  gene = rf.data$MLYCD,\n  class = rf.data$vital\n) %>%\n  ggplot(aes(x=class, y=gene))+\n  geom_boxplot() +\n  theme_minimal() +\n  panel_border() +\n  ggpubr::stat_compare_means() +\n  labs(subtitle = \"MLYCD\"),\n\n\ndata.frame(\n  gene = rf.data$POLR3GL,\n  class = rf.data$vital\n) %>%\n  ggplot(aes(x=class, y=gene))+\n  geom_boxplot() +\n  theme_minimal() +\n  panel_border() +\n  ggpubr::stat_compare_means()+\n  labs(subtitle = \"POLR3GL\"),\nnrow=1\n\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nSTIM2 has recently been found to be negatively correlated with survival in AML patients [@Djordjevic2024] and upregulation of *POLR3G* has been found correlated with unfavorable survival outcomes in some cancers [@Van2022].\n\nCaution should be used with altering the minimum depth as a parameter to be optimised, as it is dependent on the dimensionality of the dataset .\n\n### Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance_frame <- measure_importance(rf)\n```\n:::\n\n\nThe importance frame has the following columns:\n\n1.  `accuracy_decrease`Â (classification) â€“ mean decrease of prediction accuracy afterÂ ð‘‹ð‘—Â is permuted,\n\n2.  `gini_decrease`Â (classification) â€“ mean decrease in the Gini index of node impurity (i.e.Â increase of node purity) by splits onÂ ð‘‹ð‘—,\n\n3.  `mse_increase`Â (regression) â€“ mean increase of mean squared error afterÂ ð‘‹ð‘—Â is permuted,\n\n4.  `node_purity_increase`Â (regression) â€“ mean node purity increase by splits onÂ ð‘‹ð‘—, as measured by the decrease in sum of squares,\n\n5.  `mean_minimal_depth`Â â€“ mean minimal depth calculated in one of three ways specified by the parameterÂ `mean_sample`,\n\n6.  `no_of_trees`Â â€“ total number of trees in which a split onÂ ð‘‹ð‘—Â occurs,\n\n7.  `no_of_nodes`Â â€“ total number of nodes that useÂ ð‘‹ð‘—Â for splitting (it is usually equal toÂ `no_of_trees`Â if trees are shallow),\n\n8.  `times_a_root`Â â€“ total number of trees in whichÂ ð‘‹ð‘—Â is used for splitting the root node (i.e., the whole sample is divided into two based on the value ofÂ ð‘‹ð‘—),\n\n9.  `p_value`Â â€“Â ð‘-value for the one-sided binomial test using the following distribution:Â \n\n    ðµð‘–ð‘›(ðš—ðš˜_ðš˜ðš_ðš—ðš˜ðšðšŽðšœ,Â ð(node splits onÂ ð‘‹ð‘—)),\n\n    where we calculate the probability of split onÂ ð‘‹ð‘—Â as ifÂ ð‘‹ð‘—Â was uniformly drawn from theÂ ð‘ŸÂ candidate variablesÂ \n\n    ð(node splits onÂ ð‘‹ð‘—)=ð(ð‘‹ð‘—Â is a candidate)â‹…ð(ð‘‹ð‘—Â is selected)=ð‘Ÿð‘â‹…1ð‘Ÿ=1ð‘.\n\n1-4 are computed from the randomForest function, the rest are from the explainer package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_multi_way_importance(importance_frame, \n                          size_measure = \"no_of_nodes\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_multi_way_importance(importance_frame, \n                          x_measure = \"accuracy_decrease\", \n                          y_measure = \"gini_decrease\", \n                          size_measure = \"p_value\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using alpha for a discrete variable is not advised.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}