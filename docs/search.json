[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "HuMo app\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Explainer\n\n\n\nmachine learning\n\n\nbasics\n\n\nexplainer\n\n\nTCGA\n\n\nrandom forest\n\n\n\nTesting out randomForestExplainer package\n\n\n\nHolly Hall\n\n\nAug 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUMAP\n\n\n\nbasics\n\n\nUMAP\n\n\n\nReviewing the basics of UMAP\n\n\n\nHolly Hall\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\n\n\n\nbasics\n\n\nPCA\n\n\n\nReviewing the basics of Principle Component Analysis\n\n\n\nHolly Hall\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression\n\n\n\nmachine learning\n\n\nbasics\n\n\nlinear regression\n\n\nK-fold\n\n\n\nReviewing the basics of linear regression\n\n\n\nHolly Hall\n\n\nAug 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublications how-to\n\n\n\nQuarto\n\n\nR\n\n\nwebsite\n\n\nhtml\n\n\n\nHow I finally made a citation list look how I wanted it to\n\n\n\nHolly Hall\n\n\nSep 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi!",
    "section": "",
    "text": "I‚Äôm Holly, a post-doc at CRUK Beatson Institute in Prof Crispin Miller‚Äôs group.\nI‚Äôm attempting to learn how to properly use Quarto, especially lua, HTML, and CSS. They say the best way to learn is doing‚Ä¶ so here‚Äôs my attempt at doing."
  },
  {
    "objectID": "posts/20240813_RF_explainer/index.html",
    "href": "posts/20240813_RF_explainer/index.html",
    "title": "Random Forest Explainer",
    "section": "",
    "text": "Predictive models with complex structures can be effectively developed using random forest machine learning. A random forest is a type of predictive ML model that constructs a collection of decision trees, each generated from a bootstrapped sample of the data, to reach a consensus prediction. Random forests are versatile, applicable to both regression and classification tasks.\nHowever, they can sometimes operate as a ‚Äúblack box,‚Äù making it challenging to interpret their inner workings. To address this, it‚Äôs valuable to examine the model to identify which variables most significantly contribute to prediction accuracy. Here, I‚Äôll be testing out the randomForestExplainer package."
  },
  {
    "objectID": "posts/20240813_RF_explainer/index.html#purpose",
    "href": "posts/20240813_RF_explainer/index.html#purpose",
    "title": "Random Forest Explainer",
    "section": "",
    "text": "Predictive models with complex structures can be effectively developed using random forest machine learning. A random forest is a type of predictive ML model that constructs a collection of decision trees, each generated from a bootstrapped sample of the data, to reach a consensus prediction. Random forests are versatile, applicable to both regression and classification tasks.\nHowever, they can sometimes operate as a ‚Äúblack box,‚Äù making it challenging to interpret their inner workings. To address this, it‚Äôs valuable to examine the model to identify which variables most significantly contribute to prediction accuracy. Here, I‚Äôll be testing out the randomForestExplainer package."
  },
  {
    "objectID": "posts/20240813_RF_explainer/index.html#build-a-forest",
    "href": "posts/20240813_RF_explainer/index.html#build-a-forest",
    "title": "Random Forest Explainer",
    "section": "Build a forest",
    "text": "Build a forest\nFirst we‚Äôll download some TCGA data, create our target predictor (1Y survival), and remove any genes which contribute nothing (0 counts all the way!).\n\n\nShow the code\n## Load in packages\nsuppressMessages({\n  require(recount3)\n  require(dplyr)\n  require(ggplot2)\n  require(cowplot)\n  require(fst)\n  require(randomForest)\n  require(randomForestExplainer)\n})\n\n\n\n\nShow the code\n## Get the project list from recount\nprojects_all &lt;- rbind(\n  available_projects(organism = c(\"human\"))) %&gt;%\n  filter(file_source == \"tcga\")\n\n  ## Get recount project information\n  proj_info &lt;- projects_all %&gt;%\n    filter(project == \"LAML\")\n  proj_info &lt;- proj_info[1, ]\n\n  ## Download\n  rse &lt;- create_rse(project_info = proj_info, type = \"gene\")\n\n  ## save information in list\n  raw_data &lt;- list(\n    raw_counts = compute_read_counts(rse),\n    metadata = data.frame(colData(rse))\n  )\n  \n  raw_data$metadata_min &lt;-\n    data.frame(\n      vital = raw_data$metadata$tcga.xml_vital_status,\n      days = raw_data$metadata$tcga.xml_days_to_death,\n      barcode = raw_data$metadata$tcga.tcga_barcode\n    ) %&gt;%\n    mutate(vital_1y = ifelse(vital == \"Dead\" & \n                               days &lt;= 365, \"Dead\", \"Alive\")) \n  \n  table(raw_data$metadata_min$vital_1y)\n\n\n\nAlive  Dead \n   80    35 \n\n\nShow the code\n  colnames(raw_data$raw_counts) = raw_data$metadata$tcga.tcga_barcode\n  \n  remove = is.na(raw_data$metadata_min$vital_1y)\n  remove_counts = edgeR::filterByExpr(raw_data$raw_counts,\n                                      min.count = 500)\n  \n\n\nreduced_data = list(\n  raw_counts = raw_data$raw_counts[remove_counts,!remove],\n  metadata = raw_data$metadata_min[!remove, ]\n)\n\nlibrary('biomaRt')\nmart &lt;- useDataset(\"hsapiens_gene_ensembl\", useMart(\"ensembl\"))\ngene_list &lt;- getBM(filters= \"ensembl_gene_id\", \n                attributes= c(\"ensembl_gene_id\",\"hgnc_symbol\"),\n                values=limma::strsplit2(rownames(reduced_data$raw_counts), split = \"\\\\.\")[,1],\n                mart= mart)\n\ngene_list.o = data.frame(ensembl_gene_id = limma::strsplit2(rownames(reduced_data$raw_counts), split = \"\\\\.\")[,1] ) %&gt;%\n  left_join(gene_list)\n\nrownames(reduced_data$raw_counts) = ifelse(gene_list.o$hgnc_symbol == \"\", \n                                           gene_list.o$ensembl_gene_id,\n                                           gene_list.o$hgnc_symbol)\n\n\n\n\nShow the code\nrf.data = data.frame(t(reduced_data$raw_counts))\nrf.data$vital = as.factor(reduced_data$metadata$vital_1y)\n\nset.seed(104)\n\nrf = randomForest(vital ~ ., \n                  data = rf.data,\n                  ntree = 1000,\n                  localImp = T)\n\n\n\nError\nFirst, we‚Äôll look at the error rate from this model:\n\n\nShow the code\nrf$err.rate %&gt;%\n  data.frame() %&gt;%\n  tibble::rowid_to_column() %&gt;%\n  tidyr::pivot_longer(cols = 2:4) %&gt;%\n  ggplot(aes(x=rowid, y=value, colour = name)) +\n  geom_line() +\n  theme_minimal() +\n  panel_border() +\n  labs(x=\"Trees\", \n       y=\"Error\")\n\n\n\n\n\n\n\n\n\nThe OOB line is the out of bag error rate. Since a random forest is built using bootstrapped samples of the data, the OOB is the mean prediction error rate from each sample compared to the prediction without the sample present, therefore the error from unseen samples. the OBB error rate stabilises pretty quickly, so more trees will not improve the error rate.\nSimilarly, the error rates for both of the classes stabilise. However, the error for patients which are dead within the year is particularly high.\nLets have a look at the object:\n\n\nShow the code\nrf\n\n\n\nCall:\n randomForest(formula = vital ~ ., data = rf.data, ntree = 1000,      localImp = T) \n               Type of random forest: classification\n                     Number of trees: 1000\nNo. of variables tried at each split: 91\n\n        OOB estimate of  error rate: 32.17%\nConfusion matrix:\n      Alive Dead class.error\nAlive    73    7   0.0875000\nDead     30    5   0.8571429\n\n\nHere, you can see that the OOB is approximately 30%, and the error rate for Dead patients is 0.8. This is a terrible model for prediction, as you can see from the predictions that the model typically just predicts a sample to be Alive.\n\n\nShow the code\nvarImpPlot(rf,\n           sort = T,\n           n.var = 10,\n           main = \"Top 10 - Variable Importance\")\n\n\n\n\n\n\n\n\n\n\n\nTree depth\nVariables which tend to sit closer to the root node tend to be more important for prediction accuracy. Therefore, assessing the minimal depth for each variable can be a useful exercise to examine a variables importance.\n\n\nShow the code\nmin_depth_frame &lt;- min_depth_distribution(rf)\nplot_min_depth_distribution(min_depth_frame)\n\n\n\n\n\n\n\n\n\nTo look at the top three with the smallest depth:\n\n\nShow the code\nplot_grid(\n  data.frame(\n  gene = rf.data$STIM2,\n  class = rf.data$vital\n) %&gt;%\n  ggplot(aes(x=class, y=gene))+\n  geom_boxplot() +\n  theme_minimal() +\n  panel_border() +\n  ggpubr::stat_compare_means() +\n  labs(subtitle = \"STIM2\"),\n\ndata.frame(\n  gene = rf.data$MLYCD,\n  class = rf.data$vital\n) %&gt;%\n  ggplot(aes(x=class, y=gene))+\n  geom_boxplot() +\n  theme_minimal() +\n  panel_border() +\n  ggpubr::stat_compare_means() +\n  labs(subtitle = \"MLYCD\"),\n\n\ndata.frame(\n  gene = rf.data$POLR3GL,\n  class = rf.data$vital\n) %&gt;%\n  ggplot(aes(x=class, y=gene))+\n  geom_boxplot() +\n  theme_minimal() +\n  panel_border() +\n  ggpubr::stat_compare_means()+\n  labs(subtitle = \"POLR3GL\"),\nnrow=1\n\n)\n\n\n\n\n\n\n\n\n\nSTIM2 has recently been found to be negatively correlated with survival in AML patients (Djordjevic et al. 2024) and upregulation of POLR3G has been found correlated with unfavorable survival outcomes in some cancers (Van Bortle et al. 2022).\nCaution should be used with altering the minimum depth as a parameter to be optimised, as it is dependent on the dimensionality of the dataset .\n\n\nImportance\n\n\nShow the code\nimportance_frame &lt;- measure_importance(rf)\n\n\nThe importance frame has the following columns:\n\naccuracy_decrease¬†(classification) ‚Äì mean decrease of prediction accuracy after¬†ùëãùëó¬†is permuted,\ngini_decrease¬†(classification) ‚Äì mean decrease in the Gini index of node impurity (i.e.¬†increase of node purity) by splits on¬†ùëãùëó,\nmse_increase¬†(regression) ‚Äì mean increase of mean squared error after¬†ùëãùëó¬†is permuted,\nnode_purity_increase¬†(regression) ‚Äì mean node purity increase by splits on¬†ùëãùëó, as measured by the decrease in sum of squares,\nmean_minimal_depth¬†‚Äì mean minimal depth calculated in one of three ways specified by the parameter¬†mean_sample,\nno_of_trees¬†‚Äì total number of trees in which a split on¬†ùëãùëó¬†occurs,\nno_of_nodes¬†‚Äì total number of nodes that use¬†ùëãùëó¬†for splitting (it is usually equal to¬†no_of_trees¬†if trees are shallow),\ntimes_a_root¬†‚Äì total number of trees in which¬†ùëãùëó¬†is used for splitting the root node (i.e., the whole sample is divided into two based on the value of¬†ùëãùëó),\np_value¬†‚Äì¬†ùëù-value for the one-sided binomial test using the following distribution:¬†\nùêµùëñùëõ(ùöóùöò_ùöòùöè_ùöóùöòùöçùöéùöú,¬†ùêè(node splits on¬†ùëãùëó)),\nwhere we calculate the probability of split on¬†ùëãùëó¬†as if¬†ùëãùëó¬†was uniformly drawn from the¬†ùëü¬†candidate variables¬†\nùêè(node splits on¬†ùëãùëó)=ùêè(ùëãùëó¬†is a candidate)‚ãÖùêè(ùëãùëó¬†is selected)=ùëüùëù‚ãÖ1ùëü=1ùëù.\n\n1-4 are computed from the randomForest function, the rest are from the explainer package.\n\n\nShow the code\nplot_multi_way_importance(importance_frame, \n                          size_measure = \"no_of_nodes\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_multi_way_importance(importance_frame, \n                          x_measure = \"accuracy_decrease\", \n                          y_measure = \"gini_decrease\", \n                          size_measure = \"p_value\") \n\n\nWarning: Using alpha for a discrete variable is not advised."
  },
  {
    "objectID": "posts/20240806_UMAP/index.html",
    "href": "posts/20240806_UMAP/index.html",
    "title": "UMAP",
    "section": "",
    "text": "UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that excels at reducing high-dimensional data into a low-dimensional representation. Unlike PCA (Principal Component Analysis), which is linear and only effective when the first few components capture a significant portion of the data‚Äôs variation, UMAP can handle more complex, non-linear relationships within the data.\nUMAP operates by calculating similarity scores between pairs of data points in the original high-dimensional space. These similarity scores are determined based on the number of nearest neighbors each point has, capturing the local topology of the data. UMAP uses these scores to construct a fuzzy topological representation, which it then optimizes to preserve the structure of the data when it is projected into a lower-dimensional space.\nTo create the low-dimensional representation, UMAP initializes the process with a spectral embedding, a graph-based approximation, which serves as a starting point. It then iteratively adjusts the positions of the points, guided by low-dimensional similarity scores derived from a t-distribution curve, allowing UMAP to effectively maintain the local relationships of the data points.\nUMAP shares similarities with t-SNE (t-distributed Stochastic Neighbor Embedding), another popular dimensionality reduction technique. However, while t-SNE starts with a random initial representation and adjusts every point slightly during each iteration, UMAP‚Äôs initial step is more structured, using a spectral embedding. UMAP also has the flexibility to move individual points or subsets of points with each iteration, making it particularly efficient for handling large datasets.\nOne of UMAP‚Äôs strengths is its ability to balance local and global structure in the data. By adjusting the number of nearest neighbors (NN) used in the calculation, UMAP can either emphasize the local structure, revealing finer details within clusters with low NN, or capture broader trends across the dataset with higher NN, providing a more global perspective.\nThe proper description of how UMAP works can be found in the documentation, or a handy YouTube video.\nA really good explanation of the cost functions and gradient descent differences between the various dimensional reduction tools can be found here. And this paper gives an indepth comparison of the underlying maths, which I need to read properly."
  },
  {
    "objectID": "posts/20240806_UMAP/index.html#overview",
    "href": "posts/20240806_UMAP/index.html#overview",
    "title": "UMAP",
    "section": "",
    "text": "UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that excels at reducing high-dimensional data into a low-dimensional representation. Unlike PCA (Principal Component Analysis), which is linear and only effective when the first few components capture a significant portion of the data‚Äôs variation, UMAP can handle more complex, non-linear relationships within the data.\nUMAP operates by calculating similarity scores between pairs of data points in the original high-dimensional space. These similarity scores are determined based on the number of nearest neighbors each point has, capturing the local topology of the data. UMAP uses these scores to construct a fuzzy topological representation, which it then optimizes to preserve the structure of the data when it is projected into a lower-dimensional space.\nTo create the low-dimensional representation, UMAP initializes the process with a spectral embedding, a graph-based approximation, which serves as a starting point. It then iteratively adjusts the positions of the points, guided by low-dimensional similarity scores derived from a t-distribution curve, allowing UMAP to effectively maintain the local relationships of the data points.\nUMAP shares similarities with t-SNE (t-distributed Stochastic Neighbor Embedding), another popular dimensionality reduction technique. However, while t-SNE starts with a random initial representation and adjusts every point slightly during each iteration, UMAP‚Äôs initial step is more structured, using a spectral embedding. UMAP also has the flexibility to move individual points or subsets of points with each iteration, making it particularly efficient for handling large datasets.\nOne of UMAP‚Äôs strengths is its ability to balance local and global structure in the data. By adjusting the number of nearest neighbors (NN) used in the calculation, UMAP can either emphasize the local structure, revealing finer details within clusters with low NN, or capture broader trends across the dataset with higher NN, providing a more global perspective.\nThe proper description of how UMAP works can be found in the documentation, or a handy YouTube video.\nA really good explanation of the cost functions and gradient descent differences between the various dimensional reduction tools can be found here. And this paper gives an indepth comparison of the underlying maths, which I need to read properly."
  },
  {
    "objectID": "posts/20240806_UMAP/index.html#practical-examples",
    "href": "posts/20240806_UMAP/index.html#practical-examples",
    "title": "UMAP",
    "section": "Practical examples!",
    "text": "Practical examples!\nDownload the popular mnist dataset, and make a basic umap plot with standard parameters.\n\n\nShow the code\nmnist &lt;- snedata::download_mnist(\"https://ossci-datasets.s3.amazonaws.com/mnist/\")\n\nmnist &lt;- list(\n  data = mnist[,1:784],\n  label = mnist$Label\n)\n\nsuppressMessages({library(rnndescent); require(uwot); require(ggplot2); require(cowplot); require(dplyr) })\n\nset.seed(42)\nmnist_umap2 &lt;- umap2(mnist$data)\n\n\n\n\nShow the code\ndata.frame(\n  mnist_umap2,\n  label = mnist$label\n) %&gt;% \n  ggplot(aes(X1, X2, colour = label)) +\n  geom_point(size=0.5, alpha = 0.4) +\n  theme_classic()+\n  guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) +\n  panel_border(colour = \"black\")\n\n\n\n\n\n\n\n\n\n\nnumber of NN\nSince UMAP uses the number of neighbours to a given data point to determine its position, the parameter n_neighbours is very important. I‚Äôve not seen a principled way of determining the optimal number of neighbours yet, so I do it primarily through trial and error. Too few neighbours and you can lose the global structure, too many and you might lose some local structure.\nWe‚Äôll see how choosing 2, 5, 20, and 200 neighbours changes the UMAP representation:\n\n\nShow the code\ndraw_umap &lt;- function(nn = 15) {\n  umap.res = umap2(mnist$data, n_neighbors =nn )\n  data.frame(\n    umap.res,\n    label = mnist$label\n  ) %&gt;% \n    ggplot(aes(X1, X2, colour = label)) +\n    geom_point(size=0.5, alpha = 0.4) +\n    theme_classic()+\n    guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) +\n    panel_border(colour = \"black\") +\n    labs(subtitle = paste0(\"NN = \", i)) +\n    theme(axis.title = element_blank(),\n          axis.ticks = element_blank(),\n          axis.text = element_blank())\n}\n\numap.nn &lt;- list()\nfor(i in c(2,5,20,200)){\n  name = as.character(i)\n  umap.nn[[name]] &lt;- draw_umap(i)\n  \n}\n\nplot_grid(plotlist = umap.nn)\n\n\n\n\n\n\n\n\n\nWhat is clear is that only 2 neighbours have no clustering structure, and all the digits form one large cloud. 5 neighbours have more structure, but there seems to be more mixture of the clusters than 20 neighbours. At 200 it looks like we‚Äôre starting to lose definition again.\nTBC"
  },
  {
    "objectID": "posts/20240805_linear_regression/index.html",
    "href": "posts/20240805_linear_regression/index.html",
    "title": "Linear regression",
    "section": "",
    "text": "Linear regression is used to predict the real value of a dependent value (target) from one or more independent values (features).\nThe main idea is to fit a best-fit line for data."
  },
  {
    "objectID": "posts/20240805_linear_regression/index.html#purpose",
    "href": "posts/20240805_linear_regression/index.html#purpose",
    "title": "Linear regression",
    "section": "",
    "text": "Linear regression is used to predict the real value of a dependent value (target) from one or more independent values (features).\nThe main idea is to fit a best-fit line for data."
  },
  {
    "objectID": "posts/20240805_linear_regression/index.html#assumptions",
    "href": "posts/20240805_linear_regression/index.html#assumptions",
    "title": "Linear regression",
    "section": "Assumptions",
    "text": "Assumptions\nLinearity, independent, normally distributed, and homoscadasticity (constant variance).\n\nLinearity\nCan be tested via:\n\nVisual inspection of a scatter plot\nCorrelation coefficient - near to zero the values are unlikely to be linear, 1 or -1 indicates a strong linear relationship\n\n\n\nOutliers\nAssumed that there are no/few extreme values which aren‚Äôt representative of the actual relationship between the values. Can be tested by boxplots."
  },
  {
    "objectID": "posts/20240805_linear_regression/index.html#simple-linear-regression",
    "href": "posts/20240805_linear_regression/index.html#simple-linear-regression",
    "title": "Linear regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nAim\nSimple linear regression is where there is one predictor value and the goal is to create a mapping function. \\(y = \\beta_0 + \\beta_1 h\\) where \\(y\\) is a continuous value, \\(\\beta_0\\) is the intercept, and \\(\\beta_1\\) is the slope. We aim to predict the coefficients to place the line as close as possible to all the data points in the training set \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 h\\) .\n\n\nCost\n\nCost function to minimise = Residual Sum of Squares (RSE): \\(RSS = \\sum_{i} (y_i - \\hat{y}_i)^2 = \\sum_{i}(y_i -\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\\)\nUse Least Squared Error (LSE) to find the betas, such that the RSS is minimised.\nOnce the model is fitted, evaluate with Mean Squared Error (MSE) metric: the average of the difference between the actual and the predicted values. Also a good metric to compare two models on the same dataset."
  },
  {
    "objectID": "posts/20240805_linear_regression/index.html#practical-example",
    "href": "posts/20240805_linear_regression/index.html#practical-example",
    "title": "Linear regression",
    "section": "Practical example",
    "text": "Practical example\nFirst we pull in a data set to play with. We‚Äôll pull in the Boston Housing dataset, and try to predict the cost of a home (median value of owner-occupied homes in USD 1000‚Äôs: colname medv) by the average number of rooms per home.\n\n\nShow the code\nhead(BostonHousing2)\n\n\n        town tract      lon     lat medv cmedv    crim zn indus chas   nox\n1     Nahant  2011 -70.9550 42.2550 24.0  24.0 0.00632 18  2.31    0 0.538\n2 Swampscott  2021 -70.9500 42.2875 21.6  21.6 0.02731  0  7.07    0 0.469\n3 Swampscott  2022 -70.9360 42.2830 34.7  34.7 0.02729  0  7.07    0 0.469\n4 Marblehead  2031 -70.9280 42.2930 33.4  33.4 0.03237  0  2.18    0 0.458\n5 Marblehead  2032 -70.9220 42.2980 36.2  36.2 0.06905  0  2.18    0 0.458\n6 Marblehead  2033 -70.9165 42.3040 28.7  28.7 0.02985  0  2.18    0 0.458\n     rm  age    dis rad tax ptratio      b lstat\n1 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n\n\n\nQC plots\nFirst, we can have a look at a scatter plot between the predictor and the response variable:\n\n\nShow the code\nggplot(BostonHousing2, aes(medv, rm)) +\n  geom_point() +\n  theme_classic() +\n    panel_border(color = \"black\") \n\n\n\n\n\n\n\n\n\nHere you can see the suggestion of a linear increasing relationship of the cost of a home by the number of rooms.\nNext, check outliers by boxplot:\n\n\nShow the code\nplot_grid(\n  ggplot(BostonHousing2, aes(1, medv)) +\n    geom_boxplot() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    theme(axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.x = element_blank()\n          ) +\n    labs(title = \"Value\"),\n  ggplot(BostonHousing2, aes(1, rm)) +\n    geom_boxplot() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    theme(axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.x = element_blank()\n          ) +\n    labs(title = \"Ave. N. rooms\")\n    \n    \n)\n\n\n\n\n\n\n\n\n\nGenerally, outliers are shown as points which lay outside of the 25th and 75th percentile values. Looks like we have a fair few.\nCheck correlation for linear dependency:\n\n\nShow the code\ncor.test(BostonHousing2$medv, BostonHousing2$rm)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  BostonHousing2$medv and BostonHousing2$rm\nt = 21.722, df = 504, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6474346 0.7378075\nsample estimates:\n      cor \n0.6953599 \n\n\nCheck normality using a density plot:\n\n\nShow the code\nplot_grid(\n  ggplot(BostonHousing2, aes(medv)) +\n    geom_density() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    labs(title = \"Value\"),\n  ggplot(BostonHousing2, aes( rm)) +\n    geom_density() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    labs(title = \"Ave. N. rooms\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCreate model\n\n\nShow the code\n# This creates a simple linear regression model \nmodel &lt;- lm(medv ~ rm, data = BostonHousing2)\nprint(model)\n\n\n\nCall:\nlm(formula = medv ~ rm, data = BostonHousing2)\n\nCoefficients:\n(Intercept)           rm  \n    -34.671        9.102  \n\n\nLook at the model in more depth:\n\n\nShow the code\nsummary(model)\n\n\n\nCall:\nlm(formula = medv ~ rm, data = BostonHousing2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -34.671      2.650  -13.08   &lt;2e-16 ***\nrm             9.102      0.419   21.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\np-value: is the result statistically significant? Is there a relationship between the dependent and the independent variables?\nt value: how likely is the coefficient is not equal to zero by chance\nR-squared gives you the proportion of variation explained by the model (higher better).\nThe adjusted R squared is best for comparing models, as it penalises the model for the number of predictors you have in the model (higher better).\nSE and F-statistic: measures of goodness of fit (closer to zero, higher better, respectively)\n\nWe can also calculate the AIC and BIC, for more goodness of fit values.\n\n\nUse for prediction\nCreate test and train datasets\n\n\nShow the code\ntrainingIndex &lt;- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))\n\ntrain &lt;- BostonHousing2[trainingIndex,]\ntest &lt;- BostonHousing2[-trainingIndex,]\n\n\nBuild the model on the training data, and predict\n\n\nShow the code\nlmModel &lt;- lm(medv ~ rm, data = train)\npred1 &lt;- predict(lmModel, test)\n\n\nReview QC\n\n\nShow the code\nsummary(lmModel)\n\n\n\nCall:\nlm(formula = medv ~ rm, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.158  -2.792   0.109   3.117  39.096 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -33.6481     3.0676  -10.97   &lt;2e-16 ***\nrm            8.9643     0.4861   18.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.897 on 402 degrees of freedom\nMultiple R-squared:  0.4583,    Adjusted R-squared:  0.4569 \nF-statistic: 340.1 on 1 and 402 DF,  p-value: &lt; 2.2e-16\n\n\nLooks like it is significant, and the adjusted R squared are comparative to the whole dataset.\nHow do the predicted values compare to the original?\n\n\nShow the code\nactualComp &lt;- data.frame(cbind(original = test$medv, predicted = pred1))\ncor_actual &lt;- cor(actualComp)\n\nhead(actualComp)\n\n\n   original predicted\n6      28.7  23.99209\n9      16.5  16.82965\n23     15.2  21.41039\n24     14.5  18.46114\n25     15.6  19.45618\n26     13.9  16.54279\n\n\n\n\nShow the code\nmin_max_accuracy &lt;- mean(apply(actualComp, 1, min) / apply(actualComp, 1, max))  \nmape &lt;- mean(abs((actualComp$predicted - actualComp$original))/actualComp$original)  \n\n\nmean absolute percentage deviation is 0.24\n\n\nHow do we know the sampling isn‚Äôt biased?\nWe can use k-fold validation! Here, we use the same proportional split of the data, but we split the data into ‚Äúk‚Äù mutually exclusive random samples. This allows us to check that the prediction accuracy isn‚Äôt changing much, and the slopes/intercepts aren‚Äôt changing much between models.\n\n\nShow the code\nrequire(DAAG)\n\n\nLoading required package: DAAG\n\n\nShow the code\ncvResults &lt;- suppressWarnings(\n  CVlm(BostonHousing2, \n       form.lm= formula(medv ~ rm),\n       m=5, \n       dots=FALSE, \n       seed=29, \n       legend.pos=\"topleft\",  \n       printit=FALSE, \n       main=\"Small symbols are predicted values while bigger ones are actuals.\")\n  )\n\n\n\n\n\n\n\n\n\nShow the code\nattr(cvResults, 'ms')  \n\n\n[1] 43.95309\n\n\nor we can loop it:\n\n\nShow the code\nmodelList &lt;- list()\n\nfor(i in 1:5){\n  trainingIndex &lt;- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))\n\ntrain &lt;- BostonHousing2[trainingIndex,]\ntest &lt;- BostonHousing2[-trainingIndex,]\nlmModel &lt;- lm(medv ~ rm, data = train)\nmodelList[[paste0(\"fold\", i)]] &lt;- lmModel\n}\n\nunlist(lapply(modelList, function(x) AIC(x)))\n\n\n   fold1    fold2    fold3    fold4    fold5 \n2670.940 2690.242 2715.209 2648.788 2698.229 \n\n\nDoesn‚Äôt seem to change much with other splits of the data!\nWhat if we add another factor to the model like crime?\n\n\nShow the code\nmodelList1 &lt;- list()\n\nfor(i in 1:5){\n  trainingIndex &lt;- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))\n\ntrain &lt;- BostonHousing2[trainingIndex,]\ntest &lt;- BostonHousing2[-trainingIndex,]\nlmModel &lt;- lm(medv ~  rm +crim, data = train)\nmodelList1[[paste0(\"fold\", i)]] &lt;- lmModel\n}\n\nunlist(lapply(modelList1, function(x) AIC(x)))\n\n\n   fold1    fold2    fold3    fold4    fold5 \n2591.677 2619.021 2646.129 2594.893 2640.385 \n\n\nDoesn‚Äôt seem to add much benefit!"
  },
  {
    "objectID": "posts/20240808_PCA/index.html",
    "href": "posts/20240808_PCA/index.html",
    "title": "PCA",
    "section": "",
    "text": "As data gets complex with many samples/measurements, we end up with many dimensions to consider for a given dataset.\nPCA is a linear dimensionality reduction tool, utilising Singular Value Decomposition, to project many dimensions into a lower dimension space and therefore capture the most important information from it. PCA finds the best fitted line by maximising the sum of the squared distances between the projected points to the origin. It finds the best linear combination of variables to maximise distance of samples."
  },
  {
    "objectID": "posts/20240808_PCA/index.html#purpose",
    "href": "posts/20240808_PCA/index.html#purpose",
    "title": "PCA",
    "section": "",
    "text": "As data gets complex with many samples/measurements, we end up with many dimensions to consider for a given dataset.\nPCA is a linear dimensionality reduction tool, utilising Singular Value Decomposition, to project many dimensions into a lower dimension space and therefore capture the most important information from it. PCA finds the best fitted line by maximising the sum of the squared distances between the projected points to the origin. It finds the best linear combination of variables to maximise distance of samples."
  },
  {
    "objectID": "posts/20240808_PCA/index.html#practical-example",
    "href": "posts/20240808_PCA/index.html#practical-example",
    "title": "PCA",
    "section": "Practical example",
    "text": "Practical example\nLet‚Äôs use the popular Iris data set, with the goal of predicting iris species from sepal and petal information.\nHere, we have measurements of sepal length and width, as well as petal width and length, for three iris species.\n\n\nShow the code\nsuppressPackageStartupMessages({\n  require(dplyr);require(ggplot2);require(cowplot)\n})\ndata(iris)\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe can plot each variable against one another, to see how the relationship between the variables changes:\n\n\nShow the code\nplot_grid(\n  \n  ggplot(iris, aes(Sepal.Length ,Sepal.Width )) +\n  geom_point() +\n  theme_classic() +\n  panel_border(color = \"black\") +\n  ggpubr::stat_cor() ,\n  ggplot(iris, aes(Petal.Length, Petal.Width )) +\n  geom_point() +\n  theme_classic() +\n  panel_border(color = \"black\")+\n  ggpubr::stat_cor() \n\n)\n\n\n\n\n\n\n\n\n\nHere we can see that Sepal length and width are not related, but petal length and width are. This gets tedious if we have many variables, which is where PCA comes in.\nA crucial first step is checking for missing values and normalisation of the data. The data should at least be centered, but can also be scaled. Scaling is more important if the measurements using different units/scales.\n\n\nShow the code\ncolSums(is.na(iris))\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n           0            0            0            0            0 \n\n\nShow the code\niris.list &lt;- list(\n  raw = iris[,1:4],\n  scaled = scale(iris[,1:4]),\n  species = iris$Species\n  \n)\n\n\n\nPerforming PCA\n\n\nShow the code\npca = princomp(iris.list$scaled)\npca.ind = factoextra::get_pca_ind(pca)\npca.var = factoextra::get_pca_var(pca)\n\nsummary(pca)\n\n\nImportance of components:\n                          Comp.1    Comp.2     Comp.3      Comp.4\nStandard deviation     1.7026571 0.9528572 0.38180950 0.143445939\nProportion of Variance 0.7296245 0.2285076 0.03668922 0.005178709\nCumulative Proportion  0.7296245 0.9581321 0.99482129 1.000000000\n\n\nFor components have been generated, which is equal to the number of variables of the data.\nIn PCA to split the covariance (or correlation) matrix into scale parts (eigenvalues) and direction (eigevectors). Eigenvectors with scale are loadings.\nEigenvector is just a coefficient of orthogonal transformation or projection, it is devoid of ‚Äúload‚Äù within its value. ‚ÄúLoad‚Äù is (information of the amount of) variance, magnitude. PCs are extracted to explain variance of the variables. Eigenvalues are the variances of (= explained by) PCs. When we multiply eigenvector by sq.root of the eivenvalue we ‚Äúload‚Äù the bare coefficient by the amount of variance. By that virtue we make the coefficient to be the measure of association, co-variability.\nThe loadings can be considered the coefficients of the linear combination of the original variables in the dataset, from which the PCs are computed. Larger the magnitude of the values are, the more important it is to that component. Positive values indicate that it‚Äôs presence is important, whereas negative values indicate its lack of presence is significant.\n\n\nShow the code\npca$loadings\n\n\n\nLoadings:\n             Comp.1 Comp.2 Comp.3 Comp.4\nSepal.Length  0.521  0.377  0.720  0.261\nSepal.Width  -0.269  0.923 -0.244 -0.124\nPetal.Length  0.580        -0.142 -0.801\nPetal.Width   0.565        -0.634  0.524\n\n               Comp.1 Comp.2 Comp.3 Comp.4\nSS loadings      1.00   1.00   1.00   1.00\nProportion Var   0.25   0.25   0.25   0.25\nCumulative Var   0.25   0.50   0.75   1.00\n\n\nIn this example, the first component has high positive values for everything apart from sepal width, which is relatively negative. The second component has high value for width. The first PC is the linear combination PC1 = 0.52*SepalLength ‚Äì 0.27*SepalWidth + 0.58*PetalLength + 0.56*PetalWidth. You can interpret this as a contrast between the SepalWidth variable and an equally weighted sum of the other variables.\n\n\nVisualisation\n\nScree plot\nWe can first look at a scree plot. This is used to visualise the importance of each PC and can be used to determine how many components (eigenvalues) you need for a reasonably accurate representation of the original data. This is important as the visualisation of 4+ variables is rather difficult!\n\n\nShow the code\n# compute total variance\nvariance = pca$sdev^2 / sum(pca$sdev^2)\n\nrequire(ggplot2); require(cowplot)\ndata.frame(\n  PC = paste0(\"PC\", 1:4),\n  variance\n) %&gt;%\n  mutate(cumSum = cumsum(variance)) %&gt;%\n  ggplot(aes(PC, variance)) +\n  geom_hline(yintercept = 0.96, linetype = 2) +\n  geom_col() +\n  theme_classic() +\n  panel_border() +\n  geom_point(aes(y=cumSum),\n             size = 3, \n             shape = 15,\n             colour = \"deeppink\"\n             ) +\n  geom_line(aes(y=cumSum, group = 1),\n            colour = \"deeppink\")\n\n\n\n\n\n\n\n\n\nEach bar is the amount of variance for each PC, and the points represent the cumulative sum of the variance. Here, we can see that the first two components capture 96% of the variance in the dataset together (dashed line).\n\n\nProfile plot\nThis gives the correlation of each of the original variables and their contribution to each dimension.\n\n\nShow the code\npca.var$cor %&gt;%\n  data.frame() %&gt;%\n  tibble::rownames_to_column(\"Variable\") %&gt;%\n  tidyr::pivot_longer(cols = 2:5) %&gt;%\n  ggplot(aes(x=Variable, y=value, colour = name)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  theme_classic() +\n  panel_border(colour = \"black\") +\n  geom_point() +\n  geom_line(aes(group = name)) +\n  labs(y= \"Correlation\",\n       colour = \"PC\") +\n  scale_colour_manual(values = c(\"#D1495BFF\",\"#2E4057FF\",\"#66A182FF\",\"#F9771EFF\"))\n\n\n\n\n\n\n\n\n\nPC1 is highly correlated with petal length and width, as well as sepal length, but negatively correlated with sepal width. PC2 is highly correlated with sepal width, and to a lesser extent sepal length. The third and fourth have weak correlations.\nWe can also plot this as a pattern plot:\n\n\nShow the code\npca.var$cor %&gt;%\n  data.frame() %&gt;%\n  tibble::rownames_to_column(\"Variable\") %&gt;%\n  ggplot(aes(x=Dim.1, y=Dim.2)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2) +\n  theme_classic() +\n  panel_border(colour = \"black\") +\n  geom_point() +\n  ggrepel::geom_text_repel(aes(label = Variable)) \n\n\n\n\n\n\n\n\n\n\n\nScore plot\nThe score plot visualises the projection of the original data into the projected space, utilising the ‚Äúformula‚Äù of each variable contribution to the PC.\n\n\nShow the code\nggplot(pca.ind$coord, aes(Dim.1, Dim.2, colour = iris.list$species)) +\n  geom_hline(yintercept = 0, linetype=2) +\n  geom_vline(xintercept = 0, linetype=2) +\n  geom_point() +\n  theme_classic() +\n  panel_border(colour = \"black\") +\n  labs(colour = \"Species\") +\n  scale_colour_manual(values = c(\"#2E4057FF\",\"#66A182FF\", \"#D1495BFF\"))\n\n\n\n\n\n\n\n\n\nWe can see that setosa species forms its own group to the left of PC1, with versicolor and virginica to the left. The loadings can also be plotted ontop of the scores plot, otherwise known as a biplot.\n\n\nShow the code\nfactoextra::fviz_pca_biplot(pca)\n\n\n\n\n\n\n\n\n\nMore to come‚Ä¶"
  },
  {
    "objectID": "posts/20230901_publications_how_to/index.html",
    "href": "posts/20230901_publications_how_to/index.html",
    "title": "Publications how-to",
    "section": "",
    "text": "Here I was pretty lazy and utilised Quarto‚Äôs fantastic citation tool to create a nicely formatted bibtex file.\n\nClick on insert whilst using Rstudio‚Äôs visual editor - this will bring up a pop up with multiple options.\nClick on the left hand side panel on pubmed - I then searched my name and found my existing publications and added them using the little plus button\nFor my preprints - these are on research square so I searched the DOI\nI then clicked insert\nI navigated to the newly created references.bib file and opened it up\nI changed the preprints from ‚Äú@misc‚Äù to my own ‚Äú@prepr‚Äù\nI then created entries for my papers I‚Äôm writing at the moment - copy and paste one before (only need authors, title, year)(Hester and Bryan 2022)\n\nThis page will work with any bibtex file, manual or otherwise!"
  },
  {
    "objectID": "posts/20230901_publications_how_to/index.html#making-the-references-bibtex-file",
    "href": "posts/20230901_publications_how_to/index.html#making-the-references-bibtex-file",
    "title": "Publications how-to",
    "section": "",
    "text": "Here I was pretty lazy and utilised Quarto‚Äôs fantastic citation tool to create a nicely formatted bibtex file.\n\nClick on insert whilst using Rstudio‚Äôs visual editor - this will bring up a pop up with multiple options.\nClick on the left hand side panel on pubmed - I then searched my name and found my existing publications and added them using the little plus button\nFor my preprints - these are on research square so I searched the DOI\nI then clicked insert\nI navigated to the newly created references.bib file and opened it up\nI changed the preprints from ‚Äú@misc‚Äù to my own ‚Äú@prepr‚Äù\nI then created entries for my papers I‚Äôm writing at the moment - copy and paste one before (only need authors, title, year)(Hester and Bryan 2022)\n\nThis page will work with any bibtex file, manual or otherwise!"
  },
  {
    "objectID": "posts/20230901_publications_how_to/index.html#formatting-the-data-in-r",
    "href": "posts/20230901_publications_how_to/index.html#formatting-the-data-in-r",
    "title": "Publications how-to",
    "section": "Formatting the data in R",
    "text": "Formatting the data in R\nI used a mixture of dplyr (Wickham et al. 2023) for its piping and bib2df (Ottolinger 2019) because it creates a wonderful dataframe. I set separate_names = T then collected the full author name from the list-data using bib$author = unlist(lapply(bib$AUTHOR, function(x) paste(unlist(x[\"full_name\"]), collapse = \",  \"))) so that the names are formatted how I wanted.\nI wanted my name to be highlighted so I made it bold. To do this, I used gsub to add bold syntax around my name only: &lt;strong&gt; name &lt;/strong&gt;. This also worked with the markdown standard ** name **.\nI sorted all the table in order of publication, then created three different tables for my publication groups (published, preprints, in prep). Each followed the same format: filter, select the right columns, paste the right syntax. I used glue (Hester and Bryan 2022) to create the right html format.\n\n&lt;p\\&gt; text &lt;/p&gt; is html to indicate a new paragraph?\n{column title} to use a variable/column from the df\n&lt;a href={URL} target='\\_blank'&gt;{TITLE} &lt;/a&gt; for the title of the paper\n\n&lt;a&gt; is html for a link\nhref to give it the site pointer\ntarget specifies where to open the linked document\n\n\nTook me forever to work out I needed to use glue_collapse afterwards, otherwise I ended up with rogue commas on new lines."
  },
  {
    "objectID": "posts/20230901_publications_how_to/index.html#need-to-have-a-think-how-to-transfer-this-to-pdf",
    "href": "posts/20230901_publications_how_to/index.html#need-to-have-a-think-how-to-transfer-this-to-pdf",
    "title": "Publications how-to",
    "section": "Need to have a think how to transfer this to pdf",
    "text": "Need to have a think how to transfer this to pdf\n\nSeparate names flag will make it easier to exract just the first initial from the first name. This will compress some of the author lists.\n\nBut that is for another eve‚Ä¶"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Sudhir B. Malla, Ryan M. Byrne, Maxime W. Lafarge, Shania M. Corry, Natalie C. Fisher, Petros K. Tsantoulis, Megan L. Mills, Rachel A. Ridgway, Tamsin R. M. Lannagan, Arafath K. Najumudeen, Kathryn L. Gilroy, Raheleh Amirkhah, Sarah L. Maguire, Eoghan J. Mulholland, Hayley L. Belnoue-Davis, Elena Grassi, Marco Viviani, Emily Rogan, Keara L. Redmond, Svetlana Sakhnevych, Aoife J. McCooey, Courtney Bull, Emily Hoey, Nicoleta Sinevici, Holly Hall, Baharak Ahmaderaghi, Enric Domingo, Andrew Blake, Susan D. Richman, Claudio Isella, Crispin Miller, Andrea Bertotti, Livio Trusolino, Maurice B. Loughrey, Emma M. Kerr, Sabine Tejpar, Timothy S. Maughan, Mark Lawler, Andrew D. Campbell, Simon J. Leedham, Viktor H. Koelzer, Owen J. Sansom, Philip D. Dunne (2024). Pathway level subtyping identifies a slow-cycling biological phenotype associated with poor clinical outcomes in colorectal cancer. Nature Genetics\n\n\nRobert Wiesheu, Sarah C Edwards, Ann Hedley, Holly Hall, Marie Tosolini, Marcelo Gregorio Filho Fares da Silva, Nital Sumaria, Suzanne M Castenmiller, Leyma Wardak, Yasmin Optaczy, Amy Lynn, David G Hill, Alan J Hayes, Jodie Hay, Anna Kilbey, Robin Shaw, Declan Whyte, Peter J Walsh, Alison M Michie, Gerard J Graham, Anand Manoharan, Christina Halsey, Karen Blyth, Monika C Wolkers, Crispin Miller, Daniel J Pennington, Gareth W Jones, Jean-Jacques Fournie, Vasileios Bekiaris, Seth B Coffelt (2024). IL-27 maintains cytotoxic Ly6C+ Œ≥Œ¥ T cells that arise from immature precursors. The EMBO Journal\n\n\nPooyeh Farahmand, Katarina Gyuraszova, Claire Rooney, Ximena L. Raffo-Iraolagoitia, Geeshath Jayasekera, Ann Hedley, Emma Johnson, Tatyana Chernova, Gaurav Malviya, Holly Hall, Tiziana Monteverde, Kevin Blyth, Rodger Duffin, Leo M. Carlin, David Lewis, John Le Quesne, Marion MacFarlane, Daniel J. Murphy (2023). Asbestos accelerates disease onset in a genetic model of malignant pleural mesothelioma. Frontiers in Toxicology\n\n\nArafath K. Najumudeen, Fatih Ceteci, Sigrid K. Fey, Gregory Hamm, Rory T. Steven, Holly Hall, Chelsea J. Nikula, Alex Dexter, Teresa Murta, Alan M. Race, David Sumpton, Nikola Vlahov, David M. Gay, John R. P. Knight, Rene Jackstadt, Joshua D. G. Leach, Rachel A. Ridgway, Emma R. Johnson, Colin Nixon, Ann Hedley, Kathryn Gilroy, William Clark, Sudhir B. Malla, Philip D. Dunne, Giovanny Rodriguez-Blanco, Susan E. Critchlow, Agata Mrowinska, Gaurav Malviya, Dmitry Solovyev, Gavin Brown, David Y. Lewis, Gillian M. Mackay, Douglas Strathdee, Saverio Tardito, Eyal Gottlieb, Zoltan Takats, Simon T. Barry, Richard J. A. Goodwin, Josephine Bunch, Martin Bushell, Andrew D. Campbell, Owen J. Sansom (2021). The amino acid transporter SLC7A5 is required for efficient growth of KRAS-mutant colorectal cancer. Nature Genetics\n\n\nTimothy J. Humpton, Holly Hall, Christos Kiourtis, Colin Nixon, William Clark, Ann Hedley, Robin Shaw, Thomas G. Bird, Karen Blyth, Karen H. Vousden (2021). p53-mediated redox control promotes liver regeneration and maintains liver function in response to CCl4. Cell Death & Differentiation\n\n\nSelina Tsim, Laura Alexander, Caroline Kelly, Ann Shaw, Samantha Hinsley, Stephen Clark, Matthew Evison, Jayne Holme, Euan J. Cameron, Davand Sharma, Angela Wright, Seamus Grundy, Douglas Grieve, Alina Ionescu, David P. Breen, Elankumaran Paramasivam, Ioannis Psallidas, Dipak Mukherjee, Mahendran Chetty, Giles Cox, Alan Hart-Thomas, Rehan Naseer, John Edwards, Cyrus Daneshvar, Rakesh Panchal, Mohammed Munavvar, Rachel Ostroff, Leigh Alexander, Holly Hall, Matthew Neilson, Crispin Miller, Carol McCormick, Fiona Thomson, Anthony J. Chalmers, Nick A. Maskell, Kevin G. Blyth (2021). Serum Proteomics and Plasma Fibulin-3 in Differentiation of Mesothelioma From Asbestos-Exposed Controls and Patients With Other Pleural Diseases. Journal of Thoracic Oncology\n\n\nSergi Marco, Matthew Neilson, Madeleine Moore, Arantxa Perez-Garcia, Holly Hall, Louise Mitchell, Sergio Lilla, Giovani R. Blanco, Ann Hedley, Sara Zanivan, Jim C. Norman (2021). Nuclear-capture of endosomes depletes nuclear G-actin to promote SRF/MRTF activation and cancer cell invasion. Nature Communications\n\n\nLinda K Rushworth, Victoria Harle, Peter Repiscak, William Clark, Robin Shaw, Holly Hall, Martin Bushell, Hing Y Leung, Rachana Patel (2020). In vivo CRISPR/Cas9 knockout screen: TCEAL1 silencing enhances docetaxel efficacy in prostate cancer. Life Science Alliance\n\n\nAnuradha Ravi, Suheir Ereqat, Amer Al-Jawabreh, Ziad Abdeen, Omar Abu Shamma, Holly Hall, Mark J. Pallen, Abedelmajeed Nasereddin (2019). Metagenomic profiling of ticks: Identification of novel rickettsial genomes and detection of tick-borne canine parvovirus. PLOS Neglected Tropical Diseases\n\n\nGeorge Skalka, Holly Hall, Joanna Somers, Martin Bushell, Anne Willis, Michal Malewicz (2019). Leucine zipper and ICAT domain containing (LZIC) protein regulates cell cycle transitions in response to ionizing radiation. Cell Cycle\n\n\nM. McFarlane, A. Millard, Holly Hall, R. Savage, C. Constantinidou, R. Arasaradnam, C. Nwokolo (2019). Urinary volatile organic compounds and faecal microbiome profiles in colorectal cancer. Colorectal Disease"
  },
  {
    "objectID": "publications.html#published",
    "href": "publications.html#published",
    "title": "Publications",
    "section": "",
    "text": "Sudhir B. Malla, Ryan M. Byrne, Maxime W. Lafarge, Shania M. Corry, Natalie C. Fisher, Petros K. Tsantoulis, Megan L. Mills, Rachel A. Ridgway, Tamsin R. M. Lannagan, Arafath K. Najumudeen, Kathryn L. Gilroy, Raheleh Amirkhah, Sarah L. Maguire, Eoghan J. Mulholland, Hayley L. Belnoue-Davis, Elena Grassi, Marco Viviani, Emily Rogan, Keara L. Redmond, Svetlana Sakhnevych, Aoife J. McCooey, Courtney Bull, Emily Hoey, Nicoleta Sinevici, Holly Hall, Baharak Ahmaderaghi, Enric Domingo, Andrew Blake, Susan D. Richman, Claudio Isella, Crispin Miller, Andrea Bertotti, Livio Trusolino, Maurice B. Loughrey, Emma M. Kerr, Sabine Tejpar, Timothy S. Maughan, Mark Lawler, Andrew D. Campbell, Simon J. Leedham, Viktor H. Koelzer, Owen J. Sansom, Philip D. Dunne (2024). Pathway level subtyping identifies a slow-cycling biological phenotype associated with poor clinical outcomes in colorectal cancer. Nature Genetics\n\n\nRobert Wiesheu, Sarah C Edwards, Ann Hedley, Holly Hall, Marie Tosolini, Marcelo Gregorio Filho Fares da Silva, Nital Sumaria, Suzanne M Castenmiller, Leyma Wardak, Yasmin Optaczy, Amy Lynn, David G Hill, Alan J Hayes, Jodie Hay, Anna Kilbey, Robin Shaw, Declan Whyte, Peter J Walsh, Alison M Michie, Gerard J Graham, Anand Manoharan, Christina Halsey, Karen Blyth, Monika C Wolkers, Crispin Miller, Daniel J Pennington, Gareth W Jones, Jean-Jacques Fournie, Vasileios Bekiaris, Seth B Coffelt (2024). IL-27 maintains cytotoxic Ly6C+ Œ≥Œ¥ T cells that arise from immature precursors. The EMBO Journal\n\n\nPooyeh Farahmand, Katarina Gyuraszova, Claire Rooney, Ximena L. Raffo-Iraolagoitia, Geeshath Jayasekera, Ann Hedley, Emma Johnson, Tatyana Chernova, Gaurav Malviya, Holly Hall, Tiziana Monteverde, Kevin Blyth, Rodger Duffin, Leo M. Carlin, David Lewis, John Le Quesne, Marion MacFarlane, Daniel J. Murphy (2023). Asbestos accelerates disease onset in a genetic model of malignant pleural mesothelioma. Frontiers in Toxicology\n\n\nArafath K. Najumudeen, Fatih Ceteci, Sigrid K. Fey, Gregory Hamm, Rory T. Steven, Holly Hall, Chelsea J. Nikula, Alex Dexter, Teresa Murta, Alan M. Race, David Sumpton, Nikola Vlahov, David M. Gay, John R. P. Knight, Rene Jackstadt, Joshua D. G. Leach, Rachel A. Ridgway, Emma R. Johnson, Colin Nixon, Ann Hedley, Kathryn Gilroy, William Clark, Sudhir B. Malla, Philip D. Dunne, Giovanny Rodriguez-Blanco, Susan E. Critchlow, Agata Mrowinska, Gaurav Malviya, Dmitry Solovyev, Gavin Brown, David Y. Lewis, Gillian M. Mackay, Douglas Strathdee, Saverio Tardito, Eyal Gottlieb, Zoltan Takats, Simon T. Barry, Richard J. A. Goodwin, Josephine Bunch, Martin Bushell, Andrew D. Campbell, Owen J. Sansom (2021). The amino acid transporter SLC7A5 is required for efficient growth of KRAS-mutant colorectal cancer. Nature Genetics\n\n\nTimothy J. Humpton, Holly Hall, Christos Kiourtis, Colin Nixon, William Clark, Ann Hedley, Robin Shaw, Thomas G. Bird, Karen Blyth, Karen H. Vousden (2021). p53-mediated redox control promotes liver regeneration and maintains liver function in response to CCl4. Cell Death & Differentiation\n\n\nSelina Tsim, Laura Alexander, Caroline Kelly, Ann Shaw, Samantha Hinsley, Stephen Clark, Matthew Evison, Jayne Holme, Euan J. Cameron, Davand Sharma, Angela Wright, Seamus Grundy, Douglas Grieve, Alina Ionescu, David P. Breen, Elankumaran Paramasivam, Ioannis Psallidas, Dipak Mukherjee, Mahendran Chetty, Giles Cox, Alan Hart-Thomas, Rehan Naseer, John Edwards, Cyrus Daneshvar, Rakesh Panchal, Mohammed Munavvar, Rachel Ostroff, Leigh Alexander, Holly Hall, Matthew Neilson, Crispin Miller, Carol McCormick, Fiona Thomson, Anthony J. Chalmers, Nick A. Maskell, Kevin G. Blyth (2021). Serum Proteomics and Plasma Fibulin-3 in Differentiation of Mesothelioma From Asbestos-Exposed Controls and Patients With Other Pleural Diseases. Journal of Thoracic Oncology\n\n\nSergi Marco, Matthew Neilson, Madeleine Moore, Arantxa Perez-Garcia, Holly Hall, Louise Mitchell, Sergio Lilla, Giovani R. Blanco, Ann Hedley, Sara Zanivan, Jim C. Norman (2021). Nuclear-capture of endosomes depletes nuclear G-actin to promote SRF/MRTF activation and cancer cell invasion. Nature Communications\n\n\nLinda K Rushworth, Victoria Harle, Peter Repiscak, William Clark, Robin Shaw, Holly Hall, Martin Bushell, Hing Y Leung, Rachana Patel (2020). In vivo CRISPR/Cas9 knockout screen: TCEAL1 silencing enhances docetaxel efficacy in prostate cancer. Life Science Alliance\n\n\nAnuradha Ravi, Suheir Ereqat, Amer Al-Jawabreh, Ziad Abdeen, Omar Abu Shamma, Holly Hall, Mark J. Pallen, Abedelmajeed Nasereddin (2019). Metagenomic profiling of ticks: Identification of novel rickettsial genomes and detection of tick-borne canine parvovirus. PLOS Neglected Tropical Diseases\n\n\nGeorge Skalka, Holly Hall, Joanna Somers, Martin Bushell, Anne Willis, Michal Malewicz (2019). Leucine zipper and ICAT domain containing (LZIC) protein regulates cell cycle transitions in response to ionizing radiation. Cell Cycle\n\n\nM. McFarlane, A. Millard, Holly Hall, R. Savage, C. Constantinidou, R. Arasaradnam, C. Nwokolo (2019). Urinary volatile organic compounds and faecal microbiome profiles in colorectal cancer. Colorectal Disease"
  },
  {
    "objectID": "publications.html#preprints",
    "href": "publications.html#preprints",
    "title": "Publications",
    "section": "Preprints",
    "text": "Preprints\n\nGeorgios Kanellos, Chiara Giacomelli, Alexander Raven, Nikola Vlahov, Constantinos Alexandrou, Kathryn Gilroy, Kathryn Pennel, Holly Hall, June Munro, Joseph Waldron, Sheila Bryson, Douglas Strathdee, Saira Ghafoor, Colin Nixon, Rachel Ridgway, Crispin Miller, John Knight, Andrew Campbell, Joanne Edwards, Martin Bushell, Owen Samson (2024). Nucleophosmin sustains WNT-driven cell proliferation and tumour initiation in vivo. preprint\n\n\nAlexander Raven, Kathryn Gilroy, Hu Jin, Holly Leslie, Holly Hall, Rachel Ridgway, Catriona Ford, Doga Gulhan, Nikola Vlahov, Megan Mills, Nathalie Sphyris, Miryam {M√ºller}, Stephanie May, Colin Nixon, Nick Barker, Hans Clevers, Johanna Ivaska, Crispin Miller, Nigel Jamieson, Thomas Bird, Peter Park, Owen Sansom (2023). Hepatocyte identity and zonal position determine tumourigenic potential of mutant Œ≤-catenin. preprint\n\n\nMiryam {M√ºller}, Stephanie May, Holly Hall, Timothy J. Kendall, Lynn McGarry, Lauriane Blukacz, Sandro Nuciforo, Thomas Jamieson, Narisa Phinichkusolchit, Sandeep Dhayade, Jack Leslie, Joep Sprangers, Gaurav Malviya, Agata Mrowinska, Emma Johnson, Misti McCain, John Halpin, Christos Kiourtis, Anastasia Georgakopoulou, Colin Nixon, William Clark, Robin Shaw, Ann Hedley, Thomas M. Drake, Ee Hong Tan, Matt Neilson, Daniel J. Murphy, David Lewis, Helen L. Reeves, Derek A. Mann, Karen Blyth, Markus H. Heim, Leo M. Carlin, Owen J. Sansom, Crispin Miller, Thomas G. Bird (2022). Human-correlated genetic HCC models identify combination therapy for precision medicine. preprint"
  },
  {
    "objectID": "publications.html#in-progress",
    "href": "publications.html#in-progress",
    "title": "Publications",
    "section": "In progress",
    "text": "In progress\n\nHolly Hall, Britt {van Abeelen}, Joseph A. Waldon, Bashir A. Mohamed, Sergio Lilla, Matthew Nielson, Sara Zanivan, Ania Wilczynska, Crispin Miller, Martin Bushell (2024). Codon usage patterns play a central role in cell state-dependent gene expression dynamics. In prep\n\n\nHolly Hall, Andrew Papanastasiou, Karen Pickering, Mayank Sikarwar, Jennifer Morton, Crispin Miller (2024). Advancing Cross-Species Transcriptomic Comparisons: Integrating Human and Mouse Data from Large Repositories. In prep"
  }
]