{
  "hash": "5e6f2b04903242616c7ef524bb0f85f9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear regression\"\ndescription: \"Reviewing the basics of linear regression\"\nauthor: \n  - name: Holly Hall\n    url: https://mdrnao.github.io\ndate: 08-05-2024\ncategories: [machine learning, basics, linear regression, K-fold]\nbibliography: references.bib\nimage: image.png\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n## Purpose\n\nLinear regression is used to predict the real value of a dependent value (target) from one or more independent values (features)\n\n## Assumptions\n\nLinearity, independent, normally distributed, and homoscadasticity (constant variance).\n\n### Linearity\n\nCan be tested via:\n\n1.  Visual inspection of a scatter plot\n\n2.  Correlation coefficient - near to zero the values are unlikely to be linear, 1 or -1 indicates a strong linear relationship\n\n### Outliers\n\nAssumed that there are no/few extreme values which aren't representative of the actual relationship between the values. Can be tested by boxplots.\n\n## Simple linear regression\n\n### Aim\n\nSimple linear regression is where there is one predictor value and the goal is to create a mapping function. $y = \\beta_0 + \\beta_1 h$ where $y$ is a continuous value, $\\beta_0$ is the intercept, and $\\beta_1$ is the slope. We aim to predict the coefficients to place the line as close as possible to all the data points in the training set $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 h$ .\n\n### Cost\n\n-   Cost function to minimise = Residual Sum of Squares (RSE): $RSS = \\sum_{i} (y_i - \\hat{y}_i)^2 = \\sum_{i}(y_i -\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2$\n-   Use Least Squared Error (LSE) to find the betas, such that the RSS is minimised.\n-   Once the model is fitted, evaluate with Mean Squared Error (MSE) metric: the average of the difference between the actual and the predicted values. Also a good metric to compare two models on the same dataset.\n-   \n\n## Practical example\n\nFirst we pull in a data set to play with. We'll pull in the Boston Housing dataset, and try to predict the cost of a home (median value of owner-occupied homes in USD 1000's: colname medv) by the average number of rooms per home.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(BostonHousing2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        town tract      lon     lat medv cmedv    crim zn indus chas   nox\n1     Nahant  2011 -70.9550 42.2550 24.0  24.0 0.00632 18  2.31    0 0.538\n2 Swampscott  2021 -70.9500 42.2875 21.6  21.6 0.02731  0  7.07    0 0.469\n3 Swampscott  2022 -70.9360 42.2830 34.7  34.7 0.02729  0  7.07    0 0.469\n4 Marblehead  2031 -70.9280 42.2930 33.4  33.4 0.03237  0  2.18    0 0.458\n5 Marblehead  2032 -70.9220 42.2980 36.2  36.2 0.06905  0  2.18    0 0.458\n6 Marblehead  2033 -70.9165 42.3040 28.7  28.7 0.02985  0  2.18    0 0.458\n     rm  age    dis rad tax ptratio      b lstat\n1 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n```\n\n\n:::\n:::\n\n\n### QC plots\n\nFirst, we can have a look at a scatter plot between the predictor and the response variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(BostonHousing2, aes(medv, rm)) +\n  geom_point() +\n  theme_classic() +\n    panel_border(color = \"black\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nHere you can see the suggestion of a linear increasing relationship of the cost of a home by the number of rooms.\n\nNext, check outliers by boxplot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(\n  ggplot(BostonHousing2, aes(1, medv)) +\n    geom_boxplot() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    theme(axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.x = element_blank()\n          ) +\n    labs(title = \"Value\"),\n  ggplot(BostonHousing2, aes(1, rm)) +\n    geom_boxplot() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    theme(axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.x = element_blank()\n          ) +\n    labs(title = \"Ave. N. rooms\")\n    \n    \n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nGenerally, outliers are shown as points which lay outside of the 25th and 75th percentile values. Looks like we have a fair few.\n\nCheck correlation for linear dependency:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(BostonHousing2$medv, BostonHousing2$rm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  BostonHousing2$medv and BostonHousing2$rm\nt = 21.722, df = 504, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6474346 0.7378075\nsample estimates:\n      cor \n0.6953599 \n```\n\n\n:::\n:::\n\n\nCheck normality using a density plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(\n  ggplot(BostonHousing2, aes(medv)) +\n    geom_density() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    labs(title = \"Value\"),\n  ggplot(BostonHousing2, aes( rm)) +\n    geom_density() +\n    theme_classic() +\n    panel_border(color = \"black\") +\n    labs(title = \"Ave. N. rooms\")\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Create model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This creates a simple linear regression model \nmodel <- lm(medv ~ rm, data = BostonHousing2)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ rm, data = BostonHousing2)\n\nCoefficients:\n(Intercept)           rm  \n    -34.671        9.102  \n```\n\n\n:::\n:::\n\n\nLook at the model in more depth:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ rm, data = BostonHousing2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -34.671      2.650  -13.08   <2e-16 ***\nrm             9.102      0.419   21.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,\tAdjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n-   p-value: is the result statistically significant? Is there a relationship between the dependent and the independent variables?\n-   t value: how likely is the coefficient is not equal to zero by chance\n-   R-squared gives you the proportion of variation explained by the model (higher better).\n-   The adjusted R squared is best for comparing models, as it penalises the model for the number of predictors you have in the model (higher better).\n-   SE and F-statistic: measures of goodness of fit (closer to zero, higher better, respectively)\n\nWe can also calculate the AIC and BIC, for more goodness of fit values.\n\n### Use for prediction\n\nCreate test and train datasets\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrainingIndex <- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))\n\ntrain <- BostonHousing2[trainingIndex,]\ntest <- BostonHousing2[-trainingIndex,]\n```\n:::\n\n\nBuild the model on the training data, and predict\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmModel <- lm(medv ~ rm, data = train)\npred1 <- predict(lmModel, test)\n```\n:::\n\n\nReview QC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lmModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = medv ~ rm, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.248  -2.482   0.202   3.070  39.670 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -35.0881     2.9872  -11.75   <2e-16 ***\nrm            9.1385     0.4744   19.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.501 on 402 degrees of freedom\nMultiple R-squared:   0.48,\tAdjusted R-squared:  0.4787 \nF-statistic: 371.1 on 1 and 402 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nLooks like it is significant, and the adjusted R squared are comparative to the whole dataset.\n\nHow do the predicted values compare to the original?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactualComp <- data.frame(cbind(original = test$medv, predicted = pred1))\ncor_actual <- cor(actualComp)\n\nhead(actualComp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   original predicted\n6      28.7  23.67251\n25     15.6  19.04843\n29     18.4  24.26651\n47     20.0  17.78731\n50     19.4  16.10583\n86     26.6  25.50021\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmin_max_accuracy <- mean(apply(actualComp, 1, min) / apply(actualComp, 1, max))  \nmape <- mean(abs((actualComp$predicted - actualComp$original))/actualComp$original)  \n```\n:::\n\n\nmean absolute percentage deviation is 0.22\n\n### How do we know the sampling isn't biased? \n\nWe can use k-fold validation! Here, we use the same proportional split of the data, but we split the data into \"k\" mutually exclusive random samples. This allows us to check that the prediction accuracy isn't changing much, and the slopes/intercepts aren't changing much between models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(DAAG)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: DAAG\n```\n\n\n:::\n\n```{.r .cell-code}\ncvResults <- suppressWarnings(\n  CVlm(BostonHousing2, \n       form.lm= formula(medv ~ rm),\n       m=5, \n       dots=FALSE, \n       seed=29, \n       legend.pos=\"topleft\",  \n       printit=FALSE, \n       main=\"Small symbols are predicted values while bigger ones are actuals.\")\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nattr(cvResults, 'ms')  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 43.95309\n```\n\n\n:::\n:::\n\n\nor we can loop it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelList <- list()\n\nfor(i in 1:5){\n  trainingIndex <- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))\n\ntrain <- BostonHousing2[trainingIndex,]\ntest <- BostonHousing2[-trainingIndex,]\nlmModel <- lm(medv ~ rm, data = train)\nmodelList[[paste0(\"fold\", i)]] <- lmModel\n}\n\nunlist(lapply(modelList, function(x) AIC(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fold1    fold2    fold3    fold4    fold5 \n2670.940 2690.242 2715.209 2648.788 2698.229 \n```\n\n\n:::\n:::\n\n\nDoesn't seem to change much with other splits of the data!\n\nWhat if we add another factor to the model like crime?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelList1 <- list()\n\nfor(i in 1:5){\n  trainingIndex <- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))\n\ntrain <- BostonHousing2[trainingIndex,]\ntest <- BostonHousing2[-trainingIndex,]\nlmModel <- lm(medv ~  rm +crim, data = train)\nmodelList1[[paste0(\"fold\", i)]] <- lmModel\n}\n\nunlist(lapply(modelList1, function(x) AIC(x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fold1    fold2    fold3    fold4    fold5 \n2591.677 2619.021 2646.129 2594.893 2640.385 \n```\n\n\n:::\n:::\n\n\nDoesn't seem to add much benefit!\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}